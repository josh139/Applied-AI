{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d80da75-a380-4a53-ba27-381e2d92b759",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Part A – Application area review."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd808ff-a3b0-4ae0-8ddc-145ab1a596a4",
   "metadata": {},
   "source": [
    "I have chosen to explore the `Game` domain. I intend on looking specifically at the classic `Snake Game`. My aim is create an agent which controls the snake within the game. The agent should achieve the maximum score in the minimum number of steps. Having evaluated many different research papers I found that there were 3 main approaches. These 3 approaches gained the best results. I have listed these 3 approaches below. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a854f4-954f-4079-82bf-0018a77048d6",
   "metadata": {},
   "source": [
    "## Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5165a028-f256-47b0-bc74-9550c0fa0668",
   "metadata": {},
   "source": [
    "* One application using deep reinforcement learning presented a *\"Deep Q-Learning based approach for playing the Snake game\"* [1]. This application experimented with a Deep Q-Learning Network architecture which *\"uses sensor measurement data\"* [1] to train an agent. This approach makes the application *\"straight-forward (and easier to be trained)\"* [1]. *\"Several numerical simulations have proved the effectiveness of the proposed approach\"* [1]. A similar application took a more complex approach using a *\"convolutional neural network (CNN) trained with a variant of Q-learning\"* [2]. The Deep Q-Learning architecture was altered with *\"a carefully designed reward mechanism\"* [2] which provided immediate rewards, this was to achieve better training results and effective learning of correct policies. It was concluded that the applications model *\"outperforms the baseline model\"* [2]. <br>\n",
    "\n",
    "* I also found another application using Q-Learning which *\"was introduced into the game using Keras as neural network models\"* [3]. The difference with this application was that *\"SARSA was used in the training of agents\"* [3]. By using the SARAS algorithm, on-policy is utilised which means it is expected that the application will be *\"more effective and process execution become fast.*\" [3]. In conclusion the application *\"provides comparatively better results as compared to the existing techniques used\"* [3]. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ba84f9-615d-4291-8e72-a88cb499d51a",
   "metadata": {},
   "source": [
    "## Search Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3281170a-f25d-4b17-a65a-6bd13a7c8439",
   "metadata": {},
   "source": [
    "* Another common approach within this domain is to implement a Search Algorithm. I have seen a few papers which compare these algorithms. One of them draws a comparison between Best First Search, A* Search, an improved A* Search, random move, and almighty move for an AI bot. The algorithms proposed were *\"used in training the AI bot to take appropriate and best decision during the game\"* [4]. It was found that in order for the AI bot to achieve the maximum score in the minimum number of steps a combination of algorithms should be used. *\"BFS for first 4 moves, use A* with forward search for next 34 moves, and use Almighty move for the last 62 moves\"* [4]. These were the results as BFS will first find *\"the shortest distance between the snake head and the fruit\"* [4]. Next was A* with forward search as it *\"checks for the conditions after the fruit is consumed for sustenance\"* [4]. Then Almighty move as *\"There is no possibility of the failure\"* [4]. \n",
    "\n",
    "* Another paper compared Breadth-First Search, Depth First Search, A* Search, Best First Search, and Hamiltonian path. The paper backs up the reasons for the strategy given in the previous paper. Experiment 1 in this paper shows how the algorithms operate under time constraints and we can see how BFS search performs as one of the best throughout. It therefore makes sense to use BFS for the first 4 moves. Also the paper said *\"A* Search algorithm was relatively good when compared with other algorithms in the game\"* [5], (this was based off of the experiments conducted). This explains why A* is used in the next 34 moves as it's performance was the best on average. However, \"there is no possibility of the failure of Almighty move\" [4]. Almighty move is therefore complete but the number of nodes processed is high for the algorithm, much like the Hamiltonian Search. As a result, Almighty move is best suited for when we enter the end game as there is a higher possibility of dying. Therefore the choice for the last 62 moves makes sense. A separate application I discovered mentioned how A* Search Algorithm *\"cannot commit to its first move until the optimal path is found\"* [6]. This causes problems where A* *\"will not finish calculating fast enough and run right into an obstacle\"* [6]. Due to such factors an algorithm which found *\"a way of committing to a move even if a solution had not been found\"* [6] was created. In conclusion of this application it was found that the modified algorithm *\"made the most informed decision it could with any amount of time it was given and produced consistent and non-sporadic scores\"* [6]. Also, this new algorithm was *\"the most competitive out of all of the algorithms I implemented, coming closely behind A*\"* the author mentions. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8ec968-2999-4f0d-991a-99477d017839",
   "metadata": {},
   "source": [
    "## Rating Functions and an Evolutionary Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfffad3c-1bc2-49f6-9491-a8169085063c",
   "metadata": {},
   "source": [
    "* A different approach to Deep Q-Learning and Search Algorithms I found was the use of both movement rating functions and an Evolutionary Algorithm. *\"Scores given by these functions are aggregated by linear weighted sum, and the snake takes the action that leads to the highest score\"* [7]. For example, the snake has three possible actions straight, left, and right. The application I found used four rating functions smoothness, space, apple, and banana. Banana being poisonous and deducting from the score. The evolutionary algorithm would then find the best weights for these rating functions. The paper experimenting with these functions found *\"a controller using a single rating function cannot perform well in our snake game.\"* [7], *\"length-related weights significantly improves the performance of the controller\"* [7], and that *\"EA variants can evolve controllers that outperform the heuristic controller\"* [7]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78a5f30-a43b-424e-9bbf-b28bd6da275e",
   "metadata": {},
   "source": [
    "# Part B – Compare and evaluate AI techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ddee7b-15b1-4e91-b54b-00952d1751f4",
   "metadata": {},
   "source": [
    "After conducting a brief literature review it's clear there are many AI techniques that can be applied to this problem. Each technique has their own strengths and weaknesses, advantages and disadvantages. <br>\n",
    "One AI technique which can be applied to my problem is **Deep Q-Learning**. This technique is based upon Reinforcement Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a299661-613b-4e94-8d29-9cfdf215ac4b",
   "metadata": {},
   "source": [
    "<center> <img src=\"images/ReinforcementLearning_Diagram.png\"  width=\"45%\" height=\"45%\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35d48be-9799-4482-b405-43e119d8f3fe",
   "metadata": {},
   "source": [
    "As you can see within reinforcement learning you have an agent. The agent interacts with the enviroment by sensing a state. Next the agent takes an action and then recieves a reward for the specific action. For my problem the enviroment is the set of pixels inside the walls containing the apples, my agent is the snake, the state is where the snake is on the board, the actions the snake can take are straight, left, and right, and the rewards are the apples. <br>\n",
    "*\"Q-Learning is a basic form of Reinforcement Learning which uses Q-values (also called action values) to iteratively improve the behavior of the learning agent\"* [8]. Deep Q-Learning is the combination of Q-Learning and deep neural networks. <br> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eedcc59-d79b-4528-aba0-308f3361ae8f",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"images/Deep_Q-Learning_Example.png\" width=\"45%\" height=\"45%\"><img align=\"right\" src=\"images/Q-Table_vs_Deep_Q-Learning.png\" width=\"45%\" height=\"45%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2da6c32-9040-49db-b0db-f8736cc0a577",
   "metadata": {},
   "source": [
    "The advantage of Deep Q-Learning vs Q-Learning is that it *\"replaces the regular Q-table with a neural network. Rather than mapping a state-action pair to a q-value, a neural network maps input states to (action, Q-value) pairs.\"* [9]. The neural network will then approximate the optimal Q function, this is much more effective than value iteration. Another advantage is that Deep Q-Learning is able to handle complex enviroments. This is because DQN's have the *\"ability to successfully learn complex control policies directly from raw pixel inputs\"* [1]. Also, Deep Q-Learning is able to use experience replay. *\"Experience Replay is the act of storing and replaying game states (the state, action, reward, next_state) that the RL algorithm is able to learn from\"* [9]. This feature *\"smooths the training distribution over a large amount of experiences\"* [2]. <br>\n",
    "Experience replay can however be an issue as it *\"samples previous experiences randomly without considering their quality\"* [2]. It is also known to *\"slow down learning and increase the sample complexity\"* [10]. The way the DQN learns and the fact that it is a deep learning model means that *\"the same network may not converge the same way in different runs\"* [10]. Not knowing why and how we got the results we did can be an issue. A seperate issue with DQN's is that *\"overestimations can occur when the action values are inaccurate, irrespective of the source of approximation error\"* [11]. This is bad because *\"overestimations of DQN indeed lead to poorer policies\"* [11]. <br>\n",
    "Another approach which is very popular within this domain is to implement a **Search Algorithm**. During my literature review I found that many papers had combined these Search Algorithm to create a controller that can achieve the maximum score in the minimum number of steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d7c390-cb01-4e90-9557-d5861e9955ef",
   "metadata": {},
   "source": [
    "<center> <img src=\"images/Search_Algorithms.png\"  width=\"30%\" height=\"30%\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321d223b-67c6-4d44-baed-2ab2d2d3c192",
   "metadata": {},
   "source": [
    "I am able to implement these search algorithms by seperating the pixels into a maze like structure. From these squares of pixels the search algorithm will then find a path it will take to reach the apple, the path the snake takes depends on the algorithm we are using. At the start of the game it is a good idea to use an algorithm which prioritises the minimum number of steps. <br>\n",
    "Best First Search has these advantages. Best First Search *\"finds the optimal path for the snake to reach the fruit coordinates in the game\"* [5]. This algorithm is also very easy to implement. <br>\n",
    "The disadvantage is that *\"it guarantees the snake till length 4, because for length greater than 4 snake can bite itself\"* [4]. This is because it only takes into account f(n) = h(n) *\"where h(n) calculates the Manhattan distance between the adjacent coordinates to the apple\"* [5]. Then the path taken is the one with the least f(n) value.\n",
    "A* search is however a step up from this algorithm. *\"It improves the Best First Search algorithm by finding a full path to the apple and not stopping at the first move\"* [12]. By finding the full path the snake avoids dead ends on the way to the apple. This algorithm works with *\"f(n) = g(n) + h(n) where g(n) cost of the path from the source node to n, h(n) is the estimated heuristic cost from the target node to n, f(n) is the total optimal cost of a path going through node n\"* [4]. <br>\n",
    "A* is typically modified to A* foward checking however, this is for a few reasons. One reason is that *\"it only checks the \n",
    "path till the fruit is reached\"* [4]. Due to not checking moves after the path is reached it may end up in a dead end after eating the apple. <br>\n",
    "It has been found however that A∗ with forward checking is *\"much faster all the way to half the maximum score it loses efficiency from there onwards\"* [12]. Also, *\"A∗ Search can still lead a dead end if the snake is long enough\"* [12]. <br>\n",
    "Another search algorithm used is Almighty Move. This search algorithm has been known to *\"reach the maximum score\"* [12] for *\"each run\"* [12]. <br>\n",
    "However there are many issues with Almight Move. One issue is that *\"in a board with odd-numbered height and width, Almighty Move can only guarantee that the final length of the snake is greater than (h−2)(w−2), but less than (hw − 2)\"* [12]. Also *\"the number of steps required increases to a large extent, thus increasing the time complexity\"* [4] when using this algorithm. <br>\n",
    "The last way of implementing an agent I have considered involves **Rating Functions** and an **Evolutionary Algorithm**. To take this approach the agent *\"rates each of the three possible next positions of the snake head by\"* [7] a set of rating functions. An Evolutionary Algorithm will then tune the weight values of each rating function. For example, consider this representation of the Evolution Algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931e6210-7824-409c-8bdb-35b6eb0032c7",
   "metadata": {},
   "source": [
    "<center> <img src=\"images/EA_Parameters.png\"  width=\"70%\" height=\"70%\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eaa64f2-7b3d-4a7f-bf1e-5d89f7516dd0",
   "metadata": {},
   "source": [
    "Smoothness, Space, Food, and Direction Control are our rating functions and w(n) our weights.The goal of our Evolution Algorithm is to *\"find good values of eight weight parameters\"*.\n",
    "The advantage of using an Evolution Algorithm to tune weights is that *\"it could be a very tiring and time-consuming job for a human game designer to tune these weight values manually\"* [7]. With the use of an Evolution Algorithm weight tuning is able to be *\"automatically and intelligently\"* [7] complete. Also this algorithm has a *\"robust response to changing circumstance\"* [13]. For example, if I were to add a feature to the game like a poisonous food the weights would just be adjusted and the agent will adapt.  \n",
    "Some problems with this algorithm are that results aren't reproducible, this is because the weights are based upon experiences the agent has. Also it can take a long amount of time to run this algorithm one application mentioned it took *\"about 30-60 minutes\"* [7]. This also means the algorithm requires a large amount of computational power. <br>\n",
    "On the basis of these advantages and disadvantages I find the AI technique most suited for builiding my agent is the combination of search algorithms. Due to various factors such as reproducibility and the computational power required I feel this approach is more suited. I will need to evaluate my results and complete the project in within a given deadline, therefore these factors are important. Subsequently I shall be creating an agent which plays the snake game using a combination of search algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d668a0-5349-40e1-9c82-dad366bc5abd",
   "metadata": {},
   "source": [
    "# Part C – Implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dca7b95-53bd-4673-b225-d8899eff1363",
   "metadata": {},
   "source": [
    "**a).** My application is based upon the `Game` domain. I am recreating the very popular nokia snake game. The aim of the game is to *\"make the snake grow longer by directing it to the food\"* [14]. The rules for the game are that *\"You cannot stop the snake or make it go backwards\"* [14]. Also you should *\"try not to hit the walls or the tail\"* else this will result in the end of the game. The player therefore has 3 options from its position as it cannot turn back into itself. Depending on the option selected the snakes head will move in that direction and the tail will follow behind. <br>\n",
    "The implementation will be achieved by placing the snake on to a board. The board will have 2 dimensions the x and y axis. The snake will approach the apple with its head. The agent I create will use an algorithm to choose a path which will lead the snake to the apple. The agent then inputs one of the 3 options to follow the path it has selected. These options will simply take away or add to the snakes coordinates. For example, if the snake would like to move from coordinates (5, 4) to (4, 4) the option which is equal to (-1, 0) is selected by the agent. <br>\n",
    "One of the algorithms I will use is Breadth-First Search. Breadth-First Search is a complete algorithm, meaning it will find a path if there is one and report failure if there isn't. This algorithm is also optimal and will find the shortest path from the snakes head to the apple. Breadth-First Search works by exploring and traversing all the *\"nodes in the first-level and then expands in the second level and reaches the goal on its way\"* [5]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e60251c-ba26-486b-a588-956138b3a819",
   "metadata": {},
   "source": [
    "<center> <img src=\"images/Breadth-First_Search.png\"  width=\"45%\" height=\"45%\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e8b082-a4e9-429a-bf45-23f53b2aa487",
   "metadata": {},
   "source": [
    "In the example we can see how the algorithm does this. In my case the nodes would be the possible move coordinates and the goal would be the apples coordinates. I will apply this logic with the use of a queue which will append the next node and will make sure not to explore visited nodes much like the FIFO system which Breadth-First Search uses. The time complexity for this algorithm is *\"s O(bd + 1) and O(bd + 1) respectively where b is the branching factor and n is the number of levels.\"* [5]. <br>\n",
    "One other algorithm my agent will use to decide its input is A* Search. Like Breadth-First Search one of the properties of A* Search is that it is complete. Also A* Search is optimal. Unlike Breadth-First Search however, A* Search is an informed algorithm. This means that A* Search uses an admissable heuristic. An evaluation function is the heuristic which A* Search uses. Here is the evaluation function: <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6970755-99c6-4fbc-a0c4-9371453534c8",
   "metadata": {},
   "source": [
    "<center> <img src=\"images/A_Star_Search.png\"  width=\"45%\" height=\"45%\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f744df1a-daba-4efc-abf5-1fc8b5f7c5c1",
   "metadata": {},
   "source": [
    "The example shows the contours within a problem using A* Search. A* Search expands nodes in order of increasing f value, the f value being the result of the evaluation function. An admissable heuristic is one which does not overestimate this true cost. The true cost for my snake game would be the number of moves required to reach the goal state, i.e the apple.\n",
    "The last search algorithm which my agent will use is the Almighty Move. Almighty Move is *\"guaranteed a maximum score by not caring about\n",
    "how much time is needed\"* [12]. For a map NxN Almighty Move *\"guarantees that the snake will surely eat (N<sup>2</sup>-2) Fruits\"*. Almighty Move is able to ensure this by making a snake circuit within the board as seen below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5623881-f7fa-42dd-b60f-55208b4eeb54",
   "metadata": {},
   "source": [
    "<center> <img src=\"images/Almighty_Move_Circuit.png\"  width=\"30%\" height=\"30%\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b8bfa5-e551-4676-8e72-c60a31ff6a00",
   "metadata": {},
   "source": [
    "Essentially Almighty Move should visity every node at least once, taking the longest possible route. <br>\n",
    "These algorithms will therefore act as the brain of my agent and will be my input. The agent will then decide the path it should take using the algorithm I input. The snake will then convert this path of coordinates into the straight, left, and right options. Once the snake reaches the apple this process repeats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850ede28-f0a2-4bb3-a18f-a5758e932804",
   "metadata": {},
   "source": [
    "**b).** <center> <img src=\"images/Snake_Diagram.png\"  width=\"30%\" height=\"30%\"> </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e062e592-c496-4d5b-9443-9bc756147ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame, random, operator\n",
    "from time import time\n",
    "from queue import Queue\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d474ab72-372d-41a7-a958-8c6f80a3b744",
   "metadata": {},
   "source": [
    "First I `import` all the modules I will be using for my prototype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4c9977a3-18ce-4add-996c-f51f54ecb0a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 0)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pygame.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe830744-9e93-473b-9250-e9de852077dd",
   "metadata": {},
   "source": [
    "I then initialize all imported `pygame` modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "52cd306f-437b-48d6-ab98-f9b5e45e0832",
   "metadata": {},
   "outputs": [],
   "source": [
    "WIN_DIMENSION = 480\n",
    "\n",
    "BKGCOLOUR = (141, 168, 32)\n",
    "FONT = pygame.font.SysFont(None, 25)\n",
    "\n",
    "GRIDSIZE = 20\n",
    "WALL_DIMENSION = WIN_DIMENSION + GRIDSIZE\n",
    "\n",
    "UP = (0, -1)\n",
    "DOWN = (0, 1)\n",
    "LEFT = (-1, 0)\n",
    "RIGHT = (1, 0)\n",
    "NB = 0\n",
    "PATH_EXIST = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9376f5b-820d-4eba-b097-80366832c851",
   "metadata": {},
   "source": [
    "Next I declare some global variables: <br>\n",
    "    - `WIN_DIMENSION` is used for the dimensions of the screen. <br>\n",
    "    - `BKGCOLOUR` this is used for the colour of a rect I create on my surface which acts as a background. <br>\n",
    "    - `FONT` I will use this variable for rendering TrueType fonts onto my surface. <br>\n",
    "    - `GRIDSIZE` will be the size of each individual square within my grid. <br>\n",
    "    - `WALL_DIMENSION` is the size of the wall within the game. <br>\n",
    "    - `UP`, `DOWN`, `LEFT`, and `RIGHT` are used for adding and subtracting to the (x, y) coordinates of the snake.<br>\n",
    "    - `NB` is used to track the path. <br>\n",
    "    - `PATH_EXIST` is used to check if a path is found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "987f5be9-35b3-4de0-b118-7cdc31186ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Snake():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.steps = []\n",
    "        self.scores = []\n",
    "        self.score = 0\n",
    "        self.step = 0\n",
    "        self.length = 3\n",
    "        self.direction = random.choice([UP, DOWN, LEFT, RIGHT])\n",
    "        self.coordinates = [((WIN_DIMENSION / 2), (WIN_DIMENSION / 2))]        \n",
    "        self.colour = (0, 0, 0)        \n",
    "\n",
    "    def turn(self, point):\n",
    "        if (point[0] * - 1, point[1] * - 1) == self.direction:\n",
    "            return\n",
    "        else:\n",
    "            self.direction = point\n",
    "          \n",
    "    def get_head_position(self):\n",
    "        return self.coordinates[0]  \n",
    "                  \n",
    "    def move(self):\n",
    "        current = self.get_head_position()\n",
    "        \n",
    "        x, y = self.direction\n",
    "        new = (((current[0] + (x * GRIDSIZE)) % WALL_DIMENSION), (current[1] + (y * GRIDSIZE)) % WALL_DIMENSION)\n",
    "        if new:\n",
    "            self.step += 1\n",
    "        if new in self.coordinates[0:]:\n",
    "            self.reset()\n",
    "        else:\n",
    "            self.coordinates.insert(0, new)\n",
    "            if len(self.coordinates) > self.length:\n",
    "                self.coordinates.pop()\n",
    "\n",
    "    def reset(self):\n",
    "        global PATH_EXIST\n",
    "        global NB\n",
    "        global GRAPH\n",
    "        self.scores.append(self.score)\n",
    "        self.steps.append(self.step)\n",
    "        self.length = 3\n",
    "        self.coordinates = [((WIN_DIMENSION / 2), (WIN_DIMENSION / 2))]\n",
    "        self.direction = random.choice([UP, DOWN, LEFT, RIGHT])\n",
    "        self.score = 0\n",
    "        self.step = 0\n",
    "        GRAPH = create_graph(SNAKE.coordinates)\n",
    "        PATH_EXIST = False \n",
    "        NB = 0 \n",
    "        \n",
    "    def draw(self, surface):\n",
    "        for p in self.coordinates:\n",
    "            snake = pygame.Rect((p[0], p[1]), (GRIDSIZE, GRIDSIZE))\n",
    "            pygame.draw.rect(surface, self.colour, snake)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088770ee-a5de-4188-abf7-3de031d9eb6f",
   "metadata": {},
   "source": [
    "This is the class for my **Snake**. The constructor of this class stores information regarding the score, steps, length, directions, coordinates, and colour of the snake. There are also various methods: <br>\n",
    "`turn` which is for changing the direction of the snake. `get_head_position` which is for returning the coordinates of the snakes head. `move` which moves the snake in a direction. `reset` which resets the game when the game is lost. `draw` which draws the snake on to the surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7a0e469f-86b4-4caf-9a97-cdf840f96d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Apple():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.position = (0, 0)\n",
    "        self.randomize_position()\n",
    "        self.colour = (0, 0, 0)\n",
    "\n",
    "    def randomize_position(self, coordinates=[]):     \n",
    "        Xs = [item[0] for item in coordinates]\n",
    "        Ys = [item[1] for item in coordinates]       \n",
    "        try:   \n",
    "            x_choice = random.choice([i for i in range(20, 440, 20) if i not in Xs])\n",
    "            y_choice = random.choice([i for i in range(20, 440, 20) if i not in Ys])         \n",
    "        except:\n",
    "            x_choice = random.choice([i for i in range(20, 440, 20)])\n",
    "            y_choice = random.choice([i for i in range(20, 440, 20)]) \n",
    "        \n",
    "        self.position = (x_choice, y_choice)\n",
    "\n",
    "    def draw(self, surface):               \n",
    "        r = pygame.Rect((self.position[0], self.position[1]), (GRIDSIZE, GRIDSIZE))\n",
    "        pygame.draw.rect(surface, self.colour, r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28876e6-5c15-47d5-ad0d-fbbdf6a1a7aa",
   "metadata": {},
   "source": [
    "This is the class for my **Apple**. Similar to snake the constructor of this class stores basic information like the position of the apple, the ability to randomize a position within a set of coordinates, and the colour of the apple. The methods in this class include: <br>\n",
    "`randomize_position` chooses a random position for the apple which is different from the snake coordinates. `draw` which draws the apple on to the surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4d2c7eb8-24fe-49d0-b5b2-d9e0878b0b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "SNAKE = Snake()\n",
    "APPLE = Apple()\n",
    "\n",
    "path = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f3b8b5-b451-4d77-b32a-bf2d5714ae40",
   "metadata": {},
   "source": [
    "I then create **Snake()** and **Apple()** objects along with initializing a path` that is filled using the Breadth-First Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fb9a1943-df54-46cd-8b52-f028cf4616c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addition_tuple(a, b):\n",
    "    return tuple(map(operator.add, a, b))\n",
    "\n",
    "def create_graph(coordinates = []):\n",
    "    UP = (0, -20)\n",
    "    DOWN = (0, 20)\n",
    "    LEFT = (-20, 0)\n",
    "    RIGHT = (20, 0)\n",
    "    graph = {}\n",
    "    \n",
    "    for x in range(0, 461 ,20):\n",
    "        for y in range(0, 461, 20):\n",
    "            point = (x, y)\n",
    "            if(x == 0):\n",
    "                go_right = addition_tuple(point, RIGHT)\n",
    "                if(y == 0):                \n",
    "                    go_down = addition_tuple(point, DOWN)\n",
    "                    if(go_right in coordinates):\n",
    "                        if(go_down in coordinates):\n",
    "                            graph[point] = []\n",
    "                        else:\n",
    "                            graph[point] = [go_down]\n",
    "                    elif(go_down in coordinates):\n",
    "                        graph[point] = [go_down]\n",
    "                    else:\n",
    "                        graph[point] = [go_right, go_down]\n",
    "\n",
    "\n",
    "                elif(y == 460):                \n",
    "                    go_up = addition_tuple(point, UP)\n",
    "                    if(go_right in coordinates):\n",
    "                        if(go_up in coordinates):\n",
    "                            graph[point] = []\n",
    "                        else:\n",
    "                            graph[point] = [go_up]\n",
    "                    elif(go_up in coordinates):\n",
    "                        graph[point] = [go_up]\n",
    "                    else:\n",
    "                        graph[point] = [go_right, go_up]\n",
    "                else:\n",
    "                    if(go_right in coordinates):\n",
    "                        graph[point] = []\n",
    "                    else:\n",
    "                        graph[point] = [go_right]\n",
    "            elif(x == 460):\n",
    "                go_left = addition_tuple(point, LEFT)                \n",
    "                if(y == 0):\n",
    "                    go_down = addition_tuple(point, DOWN)\n",
    "                    if(go_left in coordinates):\n",
    "                        if(go_down in coordinates):\n",
    "                            graph[point] = []\n",
    "                        else:\n",
    "                            graph[point] = [go_down]\n",
    "                    elif(go_down in coordinates):\n",
    "                        graph[point] = [go_down]\n",
    "                    else:\n",
    "                        graph[point] = [go_left, go_down]\n",
    "                    graph[point] = [go_left, go_down]\n",
    "                elif(y == 460):\n",
    "                    go_up = addition_tuple(point, UP)\n",
    "                    if(go_left in coordinates):\n",
    "                        if(go_up in coordinates):\n",
    "                            graph[point] = []\n",
    "                        else:\n",
    "                            graph[point] = [go_up]\n",
    "                    elif(go_up in coordinates):\n",
    "                        graph[point] = [go_up]\n",
    "                    else:\n",
    "                        graph[point] = [go_left, go_up]\n",
    "                    graph[point] = [go_left, go_up]\n",
    "                else:\n",
    "                    if(go_left in coordinates):\n",
    "                        graph[point] = []\n",
    "                    else:\n",
    "                        graph[point] = [go_left]                    \n",
    "            elif(y == 0):\n",
    "                go_down = addition_tuple(point, DOWN)\n",
    "                if(go_down in coordinates):\n",
    "                    graph[point] = []\n",
    "                else:                \n",
    "                    graph[point] = [go_down]\n",
    "            elif(y == 460):\n",
    "                go_up = addition_tuple(point, UP)\n",
    "                if(go_up in coordinates):\n",
    "                    graph[point] = []\n",
    "                else:                \n",
    "                    graph[point] = [go_up]                \n",
    "            else:\n",
    "                go_up = addition_tuple(point, UP)\n",
    "                go_down = addition_tuple(point, DOWN)\n",
    "                go_left = addition_tuple(point, LEFT)\n",
    "                go_right = addition_tuple(point, RIGHT)\n",
    "                \n",
    "                to_check = [go_down, go_up, go_left, go_right]\n",
    "                to_add = []\n",
    "\n",
    "                for item in to_check:\n",
    "                    if item not in coordinates:\n",
    "                        to_add.append(item)\n",
    "                \n",
    "                graph[point] = to_add\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18159dab-bf90-4969-a562-3503bfe52036",
   "metadata": {},
   "source": [
    "`addition_tuple` is a simple method used for directly adding one tupple to another. This is used instead of + as to avoid concatenation.  The `create_graph` method works by looping through a matrix of i (rows) and j (columns), the method then links each point to its neighbour, it gives as a key the current point and as a value the list of its neighbours coordinates. The method also makes sure to exclude any points which neighbour one of the snakes coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a34be2d5-23c5-4c34-a4ab-a4c3ecefe88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAPH = create_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dba5b4-10d5-47b4-9651-22389b58460e",
   "metadata": {},
   "source": [
    "I then create the initial graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "50c31586-a980-43de-8102-a8b08d886f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BFS(adj_list, start_node, target_node):\n",
    "    visited = set()\n",
    "    queue = Queue()\n",
    "\n",
    "    queue.put(start_node)\n",
    "    visited.add(start_node)\n",
    "\n",
    "    parent = dict()\n",
    "    parent[start_node] = None\n",
    "\n",
    "    path_found = False\n",
    "    while not queue.empty():\n",
    "        current_node = queue.get()\n",
    "        if current_node == target_node:\n",
    "            path_found = True\n",
    "            break\n",
    "\n",
    "        for next_node in adj_list[current_node]:\n",
    "            if next_node not in visited:\n",
    "                queue.put(next_node)\n",
    "                parent[next_node] = current_node\n",
    "                visited.add(next_node)\n",
    "\n",
    "    path = []\n",
    "    new_path = []\n",
    "    if path_found:\n",
    "        path.append(target_node)\n",
    "        while parent[target_node] is not None:\n",
    "            path.append(parent[target_node]) \n",
    "            target_node = parent[target_node]\n",
    "        path.reverse()\n",
    "      \n",
    "    for k in range(len(path) - 1):\n",
    "        new_path.append(tuple(map(lambda i, j: int((i - j)/20),path[k+1],path[k])))\n",
    "    return new_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622aa3a5-3fac-438a-b00a-561290e6acbe",
   "metadata": {},
   "source": [
    "The `BFS` method selects the start_node and then adds it to the `queue` and `visited` list. The method then inserts the `current_node` into a queue if there are no remaining adjacent vertices left. Next BFS removes the first vertex from the queue and then repeats this process until the target node is found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c660866c-48a5-42f9-8075-2ef5ad0624b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collide():\n",
    "    try:\n",
    "        x = SNAKE.get_head_position()[0]\n",
    "        y = SNAKE.get_head_position()[1]\n",
    "    except:\n",
    "        print(SNAKE.get_head_position())\n",
    "    \n",
    "    if x > WIN_DIMENSION - GRIDSIZE or x < 0 or y > WIN_DIMENSION - GRIDSIZE or y < 0:\n",
    "        SNAKE.reset()\n",
    "\n",
    "def eat():\n",
    "    global PATH_EXIST\n",
    "    global NB\n",
    "    global GRAPH\n",
    "    if SNAKE.get_head_position() == APPLE.position:\n",
    "        SNAKE.length += 1\n",
    "        SNAKE.score += 1\n",
    "        APPLE.randomize_position(SNAKE.coordinates)\n",
    "        GRAPH = create_graph(SNAKE.coordinates)\n",
    "        PATH_EXIST = False \n",
    "        NB = 0           \n",
    "\n",
    "def draw_grid(surface):\n",
    "    pygame.draw.rect(surface, BKGCOLOUR, pygame.Rect(0, 0, WIN_DIMENSION, WIN_DIMENSION))\n",
    "    x = 0\n",
    "    y = 0\n",
    "    for l in range(WIN_DIMENSION):\n",
    "        x = x + GRIDSIZE\n",
    "        y = y + GRIDSIZE\n",
    "        pygame.draw.line(surface, (0, 0, 0), (x, 0), (x, WIN_DIMENSION))\n",
    "        pygame.draw.line(surface, (0, 0, 0), (0, y), (WIN_DIMENSION, y))\n",
    "\n",
    "def redraw_window():\n",
    "    surface = pygame.Surface((WIN.get_size()))\n",
    "    surface = surface.convert()\n",
    "    draw_grid(surface)\n",
    "    SNAKE.draw(surface)\n",
    "    APPLE.draw(surface)\n",
    "    scoreboard = FONT.render(\"Score {0}\".format(SNAKE.score), 1, (0, 0, 0))\n",
    "    surface.blit(scoreboard, (1, 1))\n",
    "    WIN.blit(surface, (0, 0))\n",
    "    pygame.display.flip()\n",
    "    pygame.display.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e406fce-d7ac-4763-9497-febaab03a342",
   "metadata": {},
   "source": [
    "I then begin to add the rules of my game `collide` checks for a collision either with the snake head and the snake body or the snake head and a wall. `eat` checks if the snake has eaten an apple and increments the length if it has. `draw_grid` draws the background and grid on to my surface. `redraw_window` is simply called to redraw my window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e11b867c-ea8f-4870-b460-30dfc35ce96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    global WIN\n",
    "    global NB\n",
    "    global PATH_EXIST\n",
    "\n",
    "    WIN = pygame.display.set_mode((WIN_DIMENSION, WIN_DIMENSION))\n",
    "    pygame.display.set_caption(\"Snake\")\n",
    "    clock = pygame.time.Clock()\n",
    "    fps = 100\n",
    "\n",
    "\n",
    "    run = True\n",
    "    while run == True:  \n",
    "                                      \n",
    "        clock.tick(fps)\n",
    "\n",
    "        redraw_window()  \n",
    "                  \n",
    "        if(PATH_EXIST == False):\n",
    "            start = time()\n",
    "\n",
    "            apple = APPLE.position            \n",
    "            path = BFS(GRAPH, SNAKE.get_head_position(), apple)\n",
    "\n",
    "            end = time()\n",
    "\n",
    "        if path!=[]:            \n",
    "            PATH_EXIST = True            \n",
    "            \n",
    "        if(PATH_EXIST == True):      \n",
    "            try:\n",
    "                direction = path[NB]\n",
    "            except:\n",
    "                PATH_EXIST = False\n",
    "                NB = 0\n",
    "                continue\n",
    "                \n",
    "            SNAKE.turn(direction)\n",
    "            NB += 1\n",
    "            \n",
    "        SNAKE.move()\n",
    "\n",
    "        collide()\n",
    "        eat()\n",
    "                 \n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                run = False\n",
    "                \n",
    "\n",
    "    pygame.quit()\n",
    "    # Matplotlib causing the kernel to restart in notebook? This code works fine within spyder?\n",
    "    # plt.scatter(SNAKE.steps, SNAKE.scores)\n",
    "    # plt.xlabel(\"Steps\")\n",
    "    # plt.ylabel(\"Score\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696922ab-383f-4fed-9901-985eb8bf889a",
   "metadata": {},
   "source": [
    "I finally execute the main loop of my game. I begin by setting the screen dimensions. Next I set the caption. I then add a `clock` to tick the fps, this is to limit the speed of the game. I then set run to true. Now run is true the main loop will run. The clock is then set to tick and the `redraw_window` is called to update the screen. I now check if the path exists, if this is false I measure the time of the path calculation. The apple position is then retrieved to use as the target node. Later I again check if the path exists, if it does I check which direction to go. I then move to the direction which follows the path. Once the snake is moving I then check for a collison and also if the snake has eaten an apple. Now I check if the user has clicked on the quit icon and then exit the game with quit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d36b672-37a7-49f9-a79e-6aea8a204681",
   "metadata": {},
   "source": [
    "# Part D – Testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e937b649-764b-46fc-90fa-0d1fe4571319",
   "metadata": {},
   "source": [
    "During my literature review I had taken into account various different results regarding different algorithms. It was clear that Breadth-First Search similarly to Best-First Search *\"run quite well and fast for the first few apples\"* [12]. I had also seen results which indicated that the two algorithms were the joint best performers within an expirement on a seperate paper. *\"The experiment of the algorithms was done by setting the timer of the 120 seconds and run each algorithm about three times and the height and width of the game are 300 and 300 respectively\"* [5]. In the experiment Breadth-First Search and Best-First Search were two of the top performers. Also another paper showcased a table for the *\"range of score and the range of steps required to achieve the score mentioned\"* [4] for a set of algorithms. On the table best first search displayed some of the most efficient results. <br>\n",
    "As a result, I expected that Breadth-First Search would be able to achieve the best results within a game that has a limited score and number of steps. This is because of other factors like *\"when the snake gets long it can easily lead to a dead end\"* [12], meaning the algorithm will not perform as well with a higher score limit. I conducted an experiment to prove this theory and recieved the following results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f5d4ca-33a9-45be-8ea3-eda7e0a4c7b9",
   "metadata": {},
   "source": [
    "<center> <h2> Human Run vs BFS Run </h2> </center> <img src=\"images/Human_Run.png\" align=\"left\" width=\"50%\" height=\"50%\"> <img align=\"right\" src=\"images/BFS_Run.png\" width=\"50%\" height=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2016b2-b300-4f9c-b87e-157367f2a20d",
   "metadata": {},
   "source": [
    "On the left we see 5 games by a human and on the right 5 games by the snake agent using Breadth-First Search. For the experiment I limited the score to 20 and the number of steps to 400, meaning if the score or steps were above this the game would restart. The results show that the human was unable to achieve the maximum score and on two occasions surpassed the maximum number of steps allowed. In comparison Breadth-First Search was able to achieve the maximum score 5/5 times and never surpassed the maximum number of steps. These results show that Breadth-First Search is superior to a human with such a rule set, and would lead me to believe that it would do well against other algorithms if I were to draw a comparsion. <br>\n",
    "As for the performance of my prototype without a step and score limit I recieve the following results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0038c09-f8e3-4c55-99ba-ced79cd6a189",
   "metadata": {},
   "source": [
    "<center> <img src=\"images/BFS_Final_Results.png\" width=\"50%\" height=\"50%\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ad2b58-a3fb-4866-973c-80d20e279f7f",
   "metadata": {},
   "source": [
    "We can see that the results begin to drop off after the snake gets to around 30 apples. The results are expected to drop off around this range as the algorithm *\"only considers moving the snake to the position on the board that appears to be closest to the goal, i.e. apple\"* [12]. Therefore it is *\"easy to get stuck on local minima and plateaus\"* [12]. The algorithm does however consistently pick an efficient path. I am able to tell that the snake path is efficient throughout due to the steady rise in the number of steps which is gradual and seems to be in line with the other results with little deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb78afa-db71-4e33-8df5-87609ee8b381",
   "metadata": {},
   "source": [
    "# Part E – Evaluate results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab08f44-3ec7-42ab-82c2-5faf5af0047e",
   "metadata": {},
   "source": [
    "Overall I feel as though my prototype was successful. This is because I recieved the expected results and was able to implement one of the algorithms I set out to. For future works in order to achieve better results I feel as though it would be best to use more than one algorithm. For example, I would add a score threshold where the agent uses another algorithm depending on the state of the game, i.e the score and length. I could also add a menu which asks if the user would like to play or to activate the agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3690a811-f316-499f-bc9d-467213d4c0ee",
   "metadata": {},
   "source": [
    "## <u>References</u>\n",
    "1. Sebastianelli, A., Tipaldi, M., Ullo, S.L. and Glielmo, L., (2021), A Deep Q-Learning based approach applied to the Snake game. <br>\n",
    "https://bit.ly/3rYLf58\n",
    "2. Wei, Z., et al, (2018), Autonomous agents in Snake game via deep reinforcement learning. <br>\n",
    "https://bit.ly/3E5nwDL\n",
    "3. Almalki, A.J. and Wocjan, P., (2019), Exploration of Reinforcement Learning to Play Snake Game, (pp. 377-381). <br>\n",
    "https://bit.ly/3K0p6Ku\n",
    "4. S. Sharma, S. Mishra, N. Deodhar, A. Katageri and P. Sagar, (2019), Solving The Classic Snake Game Using AI, pp. 1-4 <br>\n",
    "https://bit.ly/3n3iIYM\n",
    "5. Appaji, N.S.D., (2020), Comparison of Searching Algorithms in AI Against Human Agent in the Snake Game. <br>\n",
    "https://bit.ly/3tlV7qz\n",
    "6. Merrill P, (2011), Snake Artificial Intelligence Controller. <br>\n",
    "https://bit.ly/3n8K7IX\n",
    "7. J. Yeh, P. Su, S. Huang and T. Chiang, (2016), Snake game AI Movement rating functions and evolutionary algorithm-based optimization. <br>\n",
    "https://bit.ly/3G6AmCF\n",
    "8. Kaustav kumar Chanda, (2021), Q-Learning in Python. <br>\n",
    "https://bit.ly/32V6qv0\n",
    "9. Mike Wang, (2020), Deep Q-Learning Tutorial: minDQN. <br>\n",
    "https://bit.ly/3zDPo02\n",
    "10. Julien Vitay, Deep Reinforcement Learning - 3 Value-based methods <br>\n",
    "https://bit.ly/3q8Uf6z\n",
    "11. Van Hasselt, H., Guez, A. and Silver, D., (2016), March. Deep reinforcement learning with double q-learning. <br>\n",
    "https://bit.ly/3GeqmHt\n",
    "12. Shu Kong and Joan Aguilar Mayans, (2016), Automated Snake Game Solvers via AI Search Algorithms. <br>\n",
    "https://bit.ly/3q7aqkL\n",
    "13. Fogel, D.B., (1997), The Advantages of Evolutionary Computation. In Bcec (pp. 1-11). <br>\n",
    "https://bit.ly/33jIAZG\n",
    "14. Nokia Wiki, Nokia Games: Snake. <br>\n",
    "https://bit.ly/3r93NOi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
